{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"04 - Spectrally Normalized  GAN.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Q2W91zMySHNM"},"source":["## Spectral Norm\n","\n","Notationally, the spectral norm of a matrix $W$ is typically represented as $\\sigma(W)$. For neural network purposes, this $W$ matrix represents a weight matrix in one of the network's layers. The spectral norm of a matrix is the matrix's largest singular value, which can be obtained via singular value decomposition (SVD).\n","\n","**A Quick Refresher on SVD**\n","\n","SVD is a generalization of [eigendecomposition](https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix) and is used to factorize a matrix as $W = U\\Sigma V^\\top$, where $U, V$ are orthogonal matrices and $\\Sigma$ is a matrix of singular values on its diagonal. Note that $\\Sigma$ doesn't have to be square.\n","\n","\\begin{align*}\n","    \\Sigma = \\begin{bmatrix}\\sigma_1 & & \\\\ & \\sigma_2 \\\\ & & \\ddots \\\\ & & & \\sigma_n\\end{bmatrix}\n","\\end{align*}\n","\n","where $\\sigma_1$ and $\\sigma_n$ are the largest and smallest singular values, respectively. Intuitively, larger values correspond to larger amounts of stretching a matrix can apply to another vector. Following this notation, $\\sigma(W) = \\sigma_1$.\n","\n","**Applying SVD to Spectral Normalization**\n","\n","To spectrally normalize the weight matrix, you divide every value in the matrix by its spectral norm. As a result, a spectrally normalized matrix $\\overline{W}_{SN}$ can be expressed as\n","\n","\\begin{align*}\n","  \\overline{W}_{SN} = \\dfrac{W}{\\sigma(W)},\n","\\end{align*}\n","\n","In practice, computing the SVD of $W$ is expensive, so the authors of the SN-GAN paper do something very neat. They instead approximate the left and right singular vectors, $\\tilde{u}$ and $\\tilde{v}$ respectively, through power iteration such that $\\sigma(W) \\approx \\tilde{u}^\\top W\\tilde{v}$.\n","\n","Starting from randomly initialization, $\\tilde{u}$ and $\\tilde{v}$ are updated according to\n","\n","\\begin{align*}\n","  \\tilde{u} &:= \\dfrac{W^\\top\\tilde{u}}{||W^\\top\\tilde{u}||_2} \\\\\n","  \\tilde{v} &:= \\dfrac{W\\tilde{v}}{||W\\tilde{v}||_2}\n","\\end{align*}\n","\n","In practice, one round of iteration is sufficient to \"achieve satisfactory performance\" as per the authors.\n","\n","Don't worry if you don't completely follow this! The algorithm is conveniently implemented as `torch.nn.utils.spectral_norm` in PyTorch, so as long as you get the general gist of how it might be useful and when to use it, then you're all set.\n"]},{"cell_type":"code","metadata":{"id":"S-B36L7AjpTn"},"source":["import torch\n","from torch import nn\n","from tqdm.auto import tqdm\n","from torchvision import transforms\n","from torchvision.datasets import MNIST\n","from torchvision.utils import make_grid\n","from torch.utils.data import DataLoader\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m5lJAm2DaY_e"},"source":["def show_tensor_images(image_tensor, num_images=25, size=(1, 28, 28)):\n","    image_tensor = (image_tensor + 1) / 2\n","    image_unflat = image_tensor.detach().cpu()\n","    image_grid = make_grid(image_unflat[:num_images], nrow=5)\n","    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"80LI0O_BeH99"},"source":["### DCGAN Generator\n","\n","Since spectral normalization is only applied to the matrices in the discriminator, the generator implementation is the same as the original."]},{"cell_type":"code","metadata":{"id":"CIRaryyqeLnv"},"source":["class Generator(nn.Module):\n","    '''\n","    Generator Class\n","    Values:\n","    z_dim: the dimension of the noise vector, a scalar\n","    im_chan: the number of channels of the output image, a scalar\n","            MNIST is black-and-white, so that's our default\n","    hidden_dim: the inner dimension, a scalar\n","    '''\n","    \n","    def __init__(self, z_dim=10, im_chan=1, hidden_dim=64):\n","        super(Generator, self).__init__()\n","        self.z_dim = z_dim\n","        # Build the neural network\n","        self.gen = nn.Sequential(\n","            self.make_gen_block(z_dim, hidden_dim * 4),\n","            self.make_gen_block(hidden_dim * 4, hidden_dim * 2, kernel_size=4, stride=1),\n","            self.make_gen_block(hidden_dim * 2, hidden_dim),\n","            self.make_gen_block(hidden_dim, im_chan, kernel_size=4, final_layer=True),\n","        )\n","\n","    def make_gen_block(self, input_channels, output_channels, kernel_size=3, stride=2, final_layer=False):\n","        '''\n","        Function to return a sequence of operations corresponding to a generator block of the DCGAN, \n","        corresponding to a transposed convolution, a batchnorm (except for in the last layer), and an activation\n","        Parameters:\n","        input_channels: how many channels the input feature representation has\n","        output_channels: how many channels the output feature representation should have\n","        kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n","        stride: the stride of the convolution\n","        final_layer: whether we're on the final layer (affects activation and batchnorm)\n","        '''\n","        # Build the neural block\n","        if not final_layer:\n","            return nn.Sequential(\n","                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n","                nn.BatchNorm2d(output_channels),\n","                nn.ReLU(inplace=True),\n","            )\n","        else: # Final Layer\n","            return nn.Sequential(\n","                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n","                nn.Tanh(),\n","            )\n","\n","    def unsqueeze_noise(self, noise):\n","        '''\n","        Function for completing a forward pass of the Generator: Given a noise vector, \n","        returns a copy of that noise with width and height = 1 and channels = z_dim.\n","        Parameters:\n","        noise: a noise tensor with dimensions (batch_size, z_dim)\n","        '''\n","        return noise.view(len(noise), self.z_dim, 1, 1)\n","\n","    def forward(self, noise):\n","        '''\n","        Function for completing a forward pass of the Generator: Given a noise vector, \n","        returns a generated image.\n","        Parameters:\n","        noise: a noise tensor with dimensions (batch_size, z_dim)\n","        '''\n","        x = self.unsqueeze_noise(noise)\n","        return self.gen(x)\n","\n","def get_noise(n_samples, z_dim, device='cpu'):\n","    '''\n","    Function for creating a noise vector: Given the dimensions (n_samples, z_dim)\n","    creates a tensor of that shape filled with random numbers from the normal distribution.\n","    Parameters:\n","    n_samples: the number of samples in the batch, a scalar\n","    z_dim: the dimension of the noise vector, a scalar\n","    device: the device type\n","    '''\n","    return torch.randn(n_samples, z_dim, device=device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DPqEoOBRemUn"},"source":["### DCGAN Discriminator\n","\n","For the discriminator, you can wrap each `nn.Conv2d` with `nn.utils.spectral_norm`. In the backend, this introduces parameters for $\\tilde{u}$ and $\\tilde{v}$ in addition to $W$ so that the $W_{SN}$ can be computed as $\\tilde{u}^\\top W\\tilde{v}$ in runtime.\n","\n","Pytorch also provides a `nn.utils.remove_spectral_norm` function, which collapses the 3 separate parameters into a single explicit $\\overline{W}_{SN} := \\tilde{u}^\\top W\\tilde{v}$. You should only apply this to your convolutional layers during inference to improve runtime speed.\n","\n","It is important note that spectral norm does not eliminate the need for batch norm. Spectral norm affects the weights of each layer, while batch norm affects the activations of each layer. You can see both in a discriminator architecture, but you can also see just one of them. Hope this is something you have fun experimenting with!"]},{"cell_type":"code","metadata":{"id":"WJu0YF5TgMho"},"source":["class Discriminator(nn.Module):\n","    '''\n","    Discriminator Class\n","    Values:\n","    im_chan: the number of channels of the output image, a scalar\n","            MNIST is black-and-white (1 channel), so that's our default.\n","    hidden_dim: the inner dimension, a scalar\n","    '''\n","\n","    def __init__(self, im_chan=1, hidden_dim=16):\n","        super(Discriminator, self).__init__()\n","        self.disc = nn.Sequential(\n","            self.make_disc_block(im_chan, hidden_dim),\n","            self.make_disc_block(hidden_dim, hidden_dim * 2),\n","            self.make_disc_block(hidden_dim * 2, 1, final_layer=True),\n","        )\n","\n","    def make_disc_block(self, input_channels, output_channels, kernel_size=4, stride=2, final_layer=False):\n","        '''\n","        Function to return a sequence of operations corresponding to a discriminator block of the DCGAN, \n","        corresponding to a convolution, a batchnorm (except for in the last layer), and an activation\n","        Parameters:\n","        input_channels: how many channels the input feature representation has\n","        output_channels: how many channels the output feature representation should have\n","        kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n","        stride: the stride of the convolution\n","        final_layer: whether we're on the final layer (affects activation and batchnorm)\n","        '''\n","        \n","        # Build the neural block\n","        if not final_layer:\n","            return nn.Sequential(\n","                nn.utils.spectral_norm(nn.Conv2d(input_channels, output_channels, kernel_size, stride)),\n","                nn.BatchNorm2d(output_channels),\n","                nn.LeakyReLU(0.2, inplace=True),\n","            )\n","        else: # Final Layer\n","            return nn.Sequential(\n","                nn.utils.spectral_norm(nn.Conv2d(input_channels, output_channels, kernel_size, stride)),\n","            )\n","\n","    def forward(self, image):\n","        '''\n","        Function for completing a forward pass of the Discriminator: Given an image tensor, \n","        returns a 1-dimension tensor representing fake/real.\n","        Parameters:\n","        image: a flattened image tensor with dimension (im_dim)\n","        '''\n","        disc_pred = self.disc(image)\n","        return disc_pred.view(len(disc_pred), -1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tFMgTfTSgosg"},"source":["### Training SN-DCGAN\n","\n","You can now put everything together and train a spectrally normalized DCGAN! Here are all your parameters for initialization and optimization. "]},{"cell_type":"code","metadata":{"id":"LDn5w93Ej33c"},"source":["criterion = nn.BCEWithLogitsLoss()\n","n_epochs = 50\n","z_dim = 64\n","display_step = 500\n","batch_size = 128\n","lr = 0.0002\n","beta_1 = 0.5 \n","beta_2 = 0.999\n","device = 'cuda'\n","\n","# We tranform our image values to be between -1 and 1 (the range of the tanh activation)\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,), (0.5,)),\n","])\n","\n","dataloader = DataLoader(\n","    MNIST(\".\", download=True, transform=transform),\n","    batch_size=batch_size,\n","    shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PhSNFuTfkFnw"},"source":["gen = Generator(z_dim).to(device)\n","gen_opt = torch.optim.Adam(gen.parameters(), lr=lr, betas=(beta_1, beta_2))\n","disc = Discriminator().to(device) \n","disc_opt = torch.optim.Adam(disc.parameters(), lr=lr, betas=(beta_1, beta_2))\n","\n","# We initialize the weights to the normal distribution\n","# with mean 0 and standard deviation 0.02\n","def weights_init(m):\n","    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n","        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n","    if isinstance(m, nn.BatchNorm2d):\n","        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n","        torch.nn.init.constant_(m.bias, 0)\n","gen = gen.apply(weights_init)\n","disc = disc.apply(weights_init)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1AZCWsaSkSkd"},"source":["cur_step = 0\n","mean_generator_loss = 0\n","mean_discriminator_loss = 0\n","for epoch in range(n_epochs):\n","    # Dataloader returns the batches\n","    for real, _ in tqdm(dataloader):\n","        cur_batch_size = len(real)\n","        real = real.to(device)\n","\n","        ## Update Discriminator ##\n","        disc_opt.zero_grad()\n","        fake_noise = get_noise(cur_batch_size, z_dim, device=device)\n","        fake = gen(fake_noise)\n","        disc_fake_pred = disc(fake.detach())\n","        disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))\n","        disc_real_pred = disc(real)\n","        disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))\n","        disc_loss = (disc_fake_loss + disc_real_loss) / 2\n","\n","        # Keep track of the average discriminator loss\n","        mean_discriminator_loss += disc_loss.item() / display_step\n","        # Update gradients\n","        disc_loss.backward(retain_graph=True)\n","        # Update optimizer\n","        disc_opt.step()\n","\n","        ## Update Generator ##\n","        gen_opt.zero_grad()\n","        fake_noise_2 = get_noise(cur_batch_size, z_dim, device=device)\n","        fake_2 = gen(fake_noise_2)\n","        disc_fake_pred = disc(fake_2)\n","        gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))\n","        gen_loss.backward()\n","        gen_opt.step()\n","\n","        # Keep track of the average generator loss\n","        mean_generator_loss += gen_loss.item() / display_step\n","\n","        ## Visualization code ##\n","        if cur_step % display_step == 0 and cur_step > 0:\n","            print(f\"Step {cur_step}: Generator loss: {mean_generator_loss}, discriminator loss: {mean_discriminator_loss}\")\n","            show_tensor_images(fake)\n","            show_tensor_images(real)\n","            mean_generator_loss = 0\n","            mean_discriminator_loss = 0\n","        cur_step += 1"],"execution_count":null,"outputs":[]}]}