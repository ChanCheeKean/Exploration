{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"09 - VAE.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"cells":[{"cell_type":"code","metadata":{"id":"pNfHQwgcHLE4","executionInfo":{"status":"ok","timestamp":1620565908228,"user_tz":-480,"elapsed":866,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}}},"source":["import torch\n","import torch.nn as nn\n","from torch.distributions.normal import Normal\n","from torch.utils.data.dataloader import DataLoader\n","from torchvision import datasets, transforms\n","import matplotlib.pyplot as plt\n","plt.rcParams[\"figure.figsize\"] = (16, 8)\n","from torch.distributions.kl import kl_divergence\n","from torchvision.utils import make_grid\n","from tqdm import tqdm\n","import time"],"execution_count":70,"outputs":[]},{"cell_type":"code","metadata":{"id":"RH36dHzaQPGu","executionInfo":{"status":"ok","timestamp":1620565883810,"user_tz":-480,"elapsed":1167,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}}},"source":["def show_tensor_images(image_tensor, num_images=25, size=(1, 28, 28)):\n","    '''\n","    Function for visualizing images: Given a tensor of images, number of images, and\n","    size per image, plots and prints the images in an uniform grid.\n","    '''\n","    image_unflat = image_tensor.detach().cpu()\n","    image_grid = make_grid(image_unflat[:num_images], nrow=5)\n","    plt.axis('off')\n","    plt.imshow(image_grid.permute(1, 2, 0).squeeze())"],"execution_count":69,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jwg5tr2-Ij7J"},"source":["transform=transforms.Compose([transforms.ToTensor()])\n","mnist_dataset = datasets.MNIST('.', train=True, transform=transform, download=True)\n","train_dataloader = DataLoader(mnist_dataset, shuffle=True, batch_size=1024)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cpley5OKIVv4"},"source":["# Encoder"]},{"cell_type":"code","metadata":{"id":"zkXuZKkdkcyf","executionInfo":{"status":"ok","timestamp":1620564364634,"user_tz":-480,"elapsed":939,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}}},"source":["class Encoder(nn.Module):\n","\n","    def __init__(self, im_chan=1, output_chan=32, hidden_dim=16):\n","        super(Encoder, self).__init__()\n","        self.z_dim = output_chan\n","        self.disc = nn.Sequential(\n","            self.make_disc_block(im_chan, hidden_dim), # (m, hidden_dim, 13, 13)\n","            self.make_disc_block(hidden_dim, hidden_dim * 2), # (m, hidden_dim * 2, 5, 5)\n","            self.make_disc_block(hidden_dim * 2, output_chan * 2, final_layer=True), # (m, output_chan * 2, 1, 1)\n","        )\n","\n","    def make_disc_block(self, input_channels, output_channels, kernel_size=4, stride=2, final_layer=False):\n","        '''\n","        Function to return a sequence of operations corresponding to a encoder block of the VAE, \n","        corresponding to a convolution, a batchnorm (except for in the last layer), and an activation\n","        Parameters:\n","        input_channels: how many channels the input feature representation has\n","        output_channels: how many channels the output feature representation should have\n","        kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n","        stride: the stride of the convolution\n","        final_layer: whether we're on the final layer (affects activation and batchnorm)\n","        '''        \n","        if not final_layer:\n","            return nn.Sequential(\n","                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n","                nn.BatchNorm2d(output_channels),\n","                nn.LeakyReLU(0.2, inplace=True),\n","            )\n","        else:\n","            return nn.Sequential(\n","                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n","            )\n","\n","    def forward(self, image):\n","        '''\n","        Function for completing a forward pass of the Encoder: Given an image tensor (im_dim), \n","        returns a 1-dimension tensor representing fake/real.\n","        '''\n","        disc_pred = self.disc(image)\n","        encoding = disc_pred.view(len(disc_pred), -1) # (m, output_chan * 2)\n","        # The stddev output is treated as the log of the variance of the normal distribution by convention and for numerical stability\n","        return encoding[:, :self.z_dim], encoding[:, self.z_dim:].exp() # (m, output_chan)"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"75dAvq7xId8T","executionInfo":{"status":"ok","timestamp":1620565364044,"user_tz":-480,"elapsed":980,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"623e185e-e6be-4b94-faab-3f12a998b720"},"source":["# unit test\n","for images, _ in tqdm(train_dataloader):\n","  a_sample = images # (m, 1, 29, 29)\n","  break\n","\n","hidden_dim = 16\n","z_dim = 32\n","encoder = Encoder()\n","\n","block = encoder.make_disc_block(1, hidden_dim)\n","output = block(a_sample)\n","print(output.shape)\n","\n","block = encoder.make_disc_block(hidden_dim, hidden_dim * 2)\n","output = block(output)\n","print(output.shape)\n","\n","block = encoder.make_disc_block(hidden_dim * 2, z_dim * 2, final_layer=True)\n","output = block(output)\n","print(output.shape)\n","\n","output = output.view(len(output), -1)\n","print(output.shape)\n","\n","de_output = output[:, :z_dim]\n","print(de_output.shape)"],"execution_count":53,"outputs":[{"output_type":"stream","text":["  0%|          | 0/59 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["torch.Size([1024, 16, 13, 13])\n","torch.Size([1024, 32, 5, 5])\n","torch.Size([1024, 64, 1, 1])\n","torch.Size([1024, 64])\n","torch.Size([1024, 32])\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"C3jXMnlnIba2"},"source":["# Decoder"]},{"cell_type":"code","metadata":{"id":"CVMbKefkINPk"},"source":["class Decoder(nn.Module):\n","    '''\n","    Decoder Class\n","    Values:\n","    z_dim: the dimension of the noise vector, a scalar\n","    im_chan: the number of channels of the output image, a scalar MNIST is black-and-white, so that's our default\n","    hidden_dim: the inner dimension, a scalar\n","    '''\n","    \n","    def __init__(self, z_dim=32, im_chan=1, hidden_dim=64):\n","        super(Decoder, self).__init__()\n","        self.z_dim = z_dim\n","        self.gen = nn.Sequential(\n","            self.make_gen_block(z_dim, hidden_dim * 4), # (m, hidden_dim * 4, 3, 3)\n","            self.make_gen_block(hidden_dim * 4, hidden_dim * 2, kernel_size=4, stride=1), # (m, hidden_dim * 2, 6, 6)\n","            self.make_gen_block(hidden_dim * 2, hidden_dim), # (m, hidden_dim, 13, 13)\n","            self.make_gen_block(hidden_dim, im_chan, kernel_size=4, final_layer=True), # (m, 1, 28, 28)\n","        )\n","\n","    def make_gen_block(self, input_channels, output_channels, kernel_size=3, stride=2, final_layer=False):\n","        '''\n","        Function to return a sequence of operations corresponding to a Decoder block of the VAE, \n","        corresponding to a transposed convolution, a batchnorm (except for in the last layer), and an activation\n","        Parameters:\n","        input_channels: how many channels the input feature representation has\n","        output_channels: how many channels the output feature representation should have\n","        kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n","        stride: the stride of the convolution\n","        final_layer: whether we're on the final layer (affects activation and batchnorm)\n","        '''\n","        if not final_layer:\n","            return nn.Sequential(\n","                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n","                nn.BatchNorm2d(output_channels),\n","                nn.ReLU(inplace=True),\n","            )\n","        else:\n","            return nn.Sequential(\n","                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n","                nn.Sigmoid(),\n","            )\n","\n","    def forward(self, noise):\n","        '''\n","        Function for completing a forward pass of the Decoder: Given a noise vector, returns a generated image.\n","        '''\n","        x = noise.view(len(noise), self.z_dim, 1, 1) # (m, z_dim, 1, 1)\n","        return self.gen(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8t651TGZNSJm","executionInfo":{"status":"ok","timestamp":1620565473193,"user_tz":-480,"elapsed":1670,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"b267e0f3-7cd2-4257-fb47-546f54c007a1"},"source":["# unit test\n","hidden_dim = 64\n","z_dim = 32\n","decoder = Decoder()\n","output = de_output.view(len(de_output), z_dim, 1, 1)\n","\n","block = decoder.make_gen_block(z_dim, hidden_dim * 4)\n","output = block(output)\n","print(output.shape)\n","\n","block = decoder.make_gen_block(hidden_dim * 4, hidden_dim * 2, kernel_size=4, stride=1)\n","output = block(output)\n","print(output.shape)\n","\n","block = decoder.make_gen_block(hidden_dim * 2, hidden_dim)\n","output = block(output)\n","print(output.shape)\n","\n","block = decoder.make_gen_block(hidden_dim, 1, kernel_size=4, final_layer=True)\n","output = block(output)\n","print(output.shape)"],"execution_count":57,"outputs":[{"output_type":"stream","text":["torch.Size([1024, 256, 3, 3])\n","torch.Size([1024, 128, 6, 6])\n","torch.Size([1024, 64, 13, 13])\n","torch.Size([1024, 1, 28, 28])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WAN7heh3G8IX"},"source":["# VAE\n","\n","You can define the VAE using the encoder and decoder as follows. In the forward pass, the VAE samples from the encoder's output distribution before passing a value to the decoder. A common mistake is to pass the mean to the decoder --- this leads to blurrier images and is not the way in which VAEs are designed to be used. So, the steps you'll take are:\n","\n","1.   Real image input to encoder\n","2.   Encoder outputs mean and standard deviation\n","3.   Sample from distribution with the outputed mean and standard deviation\n","4.   Take sampled value (vector/latent) as the input to the decoder\n","5.   Get fake sample\n","6.   Use reconstruction loss between the fake output of the decoder and the original real input to the encoder (more about this later - keep reading!)\n","7.   Backpropagate through\n"]},{"cell_type":"code","metadata":{"id":"cqeSWffaHJOG","executionInfo":{"status":"ok","timestamp":1620565712095,"user_tz":-480,"elapsed":1356,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}}},"source":["class VAE(nn.Module):\n","    '''\n","    VAE Class\n","    Values:\n","    z_dim: the dimension of the noise vector, a scalar\n","    im_chan: the number of channels of the output image, a scalar MNIST is black-and-white, so that's our default\n","    hidden_dim: the inner dimension, a scalar\n","    '''\n","    \n","    def __init__(self, z_dim=32, im_chan=1, hidden_dim=64):\n","        super(VAE, self).__init__()\n","        self.z_dim = z_dim\n","        self.encode = Encoder(im_chan, z_dim) \n","        self.decode = Decoder(z_dim, im_chan) \n","\n","    def forward(self, images):\n","        '''\n","        Function for completing a forward pass of the Decoder: Given a noise vector, \n","        returns a generated image.\n","        Parameters:\n","        images: an image tensor with dimensions (batch_size, im_chan, im_height, im_width)\n","        Returns:\n","        decoding: the autoencoded image\n","        q_dist: the z-distribution of the encoding\n","        '''\n","        q_mean, q_stddev = self.encode(images) # (m, z_dim)\n","        q_dist = Normal(q_mean, q_stddev) # (m, z_dim), # (m, z_dim)\n","        # Sample once from each distribution\n","        z_sample = q_dist.rsample() \n","        decoding = self.decode(z_sample) # (m, 1, 28, 28)\n","        return decoding, q_dist"],"execution_count":59,"outputs":[]},{"cell_type":"code","metadata":{"id":"lJJG1l95PdOD","executionInfo":{"status":"ok","timestamp":1620565804607,"user_tz":-480,"elapsed":1931,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}}},"source":["# unit test\n","block = VAE()\n","a, b = block(a_sample)"],"execution_count":67,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4eG6kQsfphw3"},"source":["# Evidence Lower Bound (ELBO)\n","\n","When training a VAE, you're trying to maximize the likelihood of the real images. What this means is that you'd like the learned probability distribution to think it's likely that a real image (and the features in that real image) occurs -- as opposed to, say, random noise or weird-looking things. And you want to maximize the likelihood of the real stuff occurring and appropriately associate it with a point in the latent space distribution prior $p(z)$ (more on this below), which is where your learned latent noise vectors will live. However, finding this likelihood explicitly is mathematically intractable. So, instead, you can get a good lower bound on the likelihood, meaning you can figure out what the worst-case scenario of the likelihood is (its lower bound which *is* mathematically tractable) and try to maximize that instead. Because if you maximize its lower bound, or worst-case, then you probably are making the likelihood better too. And this neat technique is known as maximizing the Evidence Lower Bound (ELBO).\n","\n","Some notation before jumping into explaining ELBO: First, the prior latent space distribution $p(z)$ is the prior probability you have on the latent space $z$. This represents the likelihood of a given latent point in the latent space, and you know what this actually is because you set it in the beginning as a multivariate normal distribution. Additionally, $q(z)$ refers to the posterior probability, or the distribution of the encoded images. Keep in mind that when each image is passed through the encoder, its encoding is a probability distribution.\n","\n","Knowing that notation, here's the mathematical notation for the ELBO of a VAE, which is the lower bound you want to maximize: $\\mathbb{E}\\left(\\log p(x|z)\\right) + \\mathbb{E}\\left(\\log \\frac{p(z)}{q(z)}\\right)$, which is equivalent to $\\mathbb{E}\\left(\\log p(x|z)\\right) - \\mathrm{D_{KL}}(q(z|x)\\Vert p(z))$\n","\n","ELBO can be broken down into two parts: the reconstruction loss $\\mathbb{E}\\left(\\log p(x|z)\\right)$ and the KL divergence term $\\mathrm{D_{KL}}(q(z|x)\\Vert p(z))$. You'll explore each of these two terms in the next code and text sections."]},{"cell_type":"markdown","metadata":{"id":"bO2w1di3Ci6y"},"source":["### Reconstruction Loss \n","\n","Reconstruction loss refers to the distance between the real input image (that you put into the encoder) and the generated image (that comes out of the decoder). Explicitly, the reconstruction loss term is $\\mathbb{E}\\left(\\log p(x|z)\\right)$, the log probability of the true image given the latent value. \n","\n","For MNIST, you can treat each grayscale prediction as a binary random variable (also known as a Bernoulli distribution) with the value between 0 and 1 of a pixel corresponding to the output brightness, so you can use the binary cross entropy loss between the real input image and the generated image in order to represent the reconstruction loss term. \n","\n","In general, different assumptions about the \"distribution\" of the pixel brightnesses in an image will lead to different loss functions. For example, if you assume that the brightnesses of the pixels actually follow a normal distribution instead of a binary random (Bernoulli) distribution, this corresponds to a mean squared error (MSE) reconstruction loss.\n","\n","Why the mean squared error? Well, as a point moves away from the center, $\\mu$, of a normal distribution, its negative log likelihood increases quadratically. You can also write this as $\\mathrm{NLL}(x) \\propto (x-\\mu)^2$ for $x \\sim \\mathcal{N}(\\mu,\\sigma)$. As a result, assuming the pixel brightnesses are normally distributed implies an MSE reconstruction loss. "]},{"cell_type":"code","metadata":{"id":"g0INzSqbE1-n","executionInfo":{"status":"ok","timestamp":1620565839583,"user_tz":-480,"elapsed":835,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}}},"source":["reconstruction_loss = nn.BCELoss(reduction='sum')"],"execution_count":68,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"evBse4QYCg8-"},"source":["### KL Divergence \n","\n","KL divergence, mentioned in a video (on Inception Score) last week, allows you to evaluate how different one probability distribution is from another. If you have two distributions and they are exactly the same, then KL divergence is equal to 0. KL divergence is close to the notion of distance between distributions, but notice that it's called a divergence, not a distance; this is because it is not symmetric, meaning that $\\mathrm{KL}(X\\Vert Y)$ is usually not equal to the terms flipped $\\mathrm{KL}(Y\\Vert X)$. In contrast, a true distance function, like the Euclidean distance where you would take the squared difference between two points, is symmetric where you compare $(A-B)^2$ and $(B-A)^2$. \n","\n","Now, you care about two distributions and finding how different they are: (1) the learned latent space $q(z|x)$ that your encoder is trying to model and (2) your prior on the latent space $p(z)$, which you want your learned latent space to be as close as possible to. If both of your distributions are normal distributions, you can calculate the KL divergence, or $\\mathrm{D_{KL}}(q(z|x)\\Vert p(z))$, based on a simple formula. This makes KL divergence an attractive measure to use and the normal distribution a simultaneously attractive distribution to assume on your model and data. \n","\n","Well, your encoder is learning $q(z|x)$, but what's your latent prior $p(z)$? It is actually a fairly simple distribution for the latent space with a mean of zero and a standard deviation of one in each dimension, or $\\mathcal{N}(0, I)$. You might also come across this as the *spherical normal distribution*, where the $I$ in $\\mathcal{N}(0, I)$ stands for the identity matrix, meaning its covariance is 1 along the entire diagonal of the matrix and if you like geometry, it forms a nice symmetric-looking hypersphere, or a sphere with many (here, $z$) dimensions."]},{"cell_type":"code","metadata":{"id":"OgOvg9QjDKLm","executionInfo":{"status":"ok","timestamp":1620566033909,"user_tz":-480,"elapsed":1258,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}}},"source":["def kl_divergence_loss(q_dist):\n","    return kl_divergence(\n","        q_dist, Normal(torch.zeros_like(q_dist.mean), torch.ones_like(q_dist.stddev))\n","    ).sum(-1)"],"execution_count":72,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wBvN6I4ckaN1"},"source":["# Model Training"]},{"cell_type":"code","metadata":{"id":"HVVD8HWcMOfb"},"source":["device = 'cuda'\n","vae = VAE().to(device)\n","vae_opt = torch.optim.Adam(vae.parameters(), lr=0.002)\n","for epoch in range(10):\n","    print(f\"Epoch {epoch}\")\n","    time.sleep(0.5)\n","    for images, _ in tqdm(train_dataloader):\n","        images = images.to(device)\n","        # Clear out the gradients\n","        vae_opt.zero_grad() \n","        recon_images, encoding = vae(images)\n","        loss = reconstruction_loss(recon_images, images) + kl_divergence_loss(encoding).sum()\n","        loss.backward()\n","        vae_opt.step()\n","    plt.subplot(1,2,1)\n","    show_tensor_images(images)\n","    plt.title(\"True\")\n","    plt.subplot(1,2,2)\n","    show_tensor_images(recon_images)\n","    plt.title(\"Reconstructed\")\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iHKs9dkcRWz_"},"source":["# Evaluation"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":395},"id":"xMEwapXJT4oA","executionInfo":{"status":"ok","timestamp":1620567070247,"user_tz":-480,"elapsed":1102,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"923a52f6-8e34-4276-e9b0-f8f21ae5b0bb"},"source":["sample = torch.randn((10, 32), device='cuda')\n","test = vae.decode(sample)\n","show_tensor_images(test)"],"execution_count":111,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 1152x576 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"WwcUxHYHRapC"},"source":["# Reference"]},{"cell_type":"markdown","metadata":{"id":"sknjTtpFkqXr"},"source":["*   [$\\beta$-VAEs](https://openreview.net/forum?id=Sy2fzU9gl) showed that you can weight the KL-divergence term differently to reward \"exploration\" by the model. \n","*   [VQ-VAE-2](https://arxiv.org/pdf/1906.00446.pdf) is a VAE-Autoregressive hybrid generative model, and has been ablbe to generate incredibly diverse images - keeping up with GANs. :) \n","*   [VAE-GAN](https://arxiv.org/abs/1512.09300) is a VAE-GAN hybrid generative model that uses an adversarial loss (that is, the discriminator's judgments on real/fake) on a VAE. \n","\n"]}]}