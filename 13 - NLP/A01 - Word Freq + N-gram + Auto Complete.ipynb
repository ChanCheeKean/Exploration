{"nbformat":4,"nbformat_minor":0,"metadata":{"coursera":{"schema_names":["NLPC2-3"]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"A01 - Word Freq + N-gram + Auto Complete.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"qmij6aCdZRDs"},"source":["# Language Models: Auto-Complete"]},{"cell_type":"markdown","metadata":{"id":"Im6tclNtZRD3"},"source":["A key building block for an auto-complete system is a language model.\n","A language model assigns the probability to a sequence of words, in a way that more \"likely\" sequences receive higher scores.  For example, \n",">\"I have a pen\" \n","is expected to have a higher probability than \n",">\"I am a pen\"\n","since the first one seems to be a more natural sentence in the real world."]},{"cell_type":"code","metadata":{"id":"ReKtxCVvaV6C"},"source":["pip install trax"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nYqLZUScZfDx","executionInfo":{"status":"ok","timestamp":1618673094822,"user_tz":-480,"elapsed":41199,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"1ad113ed-1b3a-419f-966c-f92d5188b914"},"source":["from google.colab import drive\n","drive.mount(\"/content/gdrive\")\n","%cd /content/gdrive/MyDrive/Colab Notebooks/13 - NLP/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n","/content/gdrive/MyDrive/Colab Notebooks/13 - NLP\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LMV8ANhhZRD7","executionInfo":{"status":"ok","timestamp":1618673123577,"user_tz":-480,"elapsed":69945,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"be2ff717-1081-41d8-865e-9b40ec3d4e37"},"source":["import math\n","import random\n","import numpy as np\n","import pandas as pd\n","from collections import defaultdict\n","import nltk\n","from trax import layers as tl\n","nltk.download('punkt')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"fytxdItzZRD-"},"source":["# Data Pre-processing"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_Sh5rMJbZRD_","executionInfo":{"status":"ok","timestamp":1618673124962,"user_tz":-480,"elapsed":71321,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"6e706f50-58fa-4a82-cb0d-4ff23d3e3436"},"source":["with open(\"./data/en_US.twitter.txt\", \"r\") as f:\n","    data = f.read()\n","len(data)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3335477"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"WxZklRGyZRD_"},"source":["### Handling 'Out of Vocabulary' words\n","\n","If your model encounters a word that it never saw during training, it won't have an input word to help it determine the next word. The model will not be able to predict the next word. \n","- This 'new' word is called an 'unknown word', or <b>out of vocabulary (OOV)</b> words.\n","- The percentage of unknown words in the test set is called the <b> OOV </b> rate. \n","\n","To handle unknown words during prediction, use a special token to represent all unknown words 'unk'. \n","- Modify the training data so that it has some 'unknown' words to train on.\n","- Words to convert into \"unknown\" words are those that do not occur very frequently in the training set."]},{"cell_type":"code","metadata":{"id":"2dq_RcPEZREA"},"source":["def get_tokenized_data(data):\n","    sentences = data.split('\\n')  \n","    tokenized_sentences = []\n","\n","    # Remove leading and trailing spaces from each sentence\n","    sentences = [s.strip() for s in sentences]\n","    sentences = [s for s in sentences if len(s) > 0]\n","    \n","    # Go through each sentence\n","    for sentence in sentences:\n","        sentence = sentence.lower()\n","        tokenized = nltk.word_tokenize(sentence)\n","        tokenized_sentences.append(tokenized)\n","    return tokenized_sentences\n","\n","def count_words(tokenized_sentences):\n","    '''Get Frequency of Each Word'''\n","    word_counts = defaultdict(int)\n","    \n","    # Loop through each sentence\n","    for sentence in tokenized_sentences:\n","        for token in sentence:\n","            word_counts[token] += 1\n","    return word_counts\n","\n","def get_words_with_nplus_frequency(tokenized_sentences, count_threshold):\n","    '''Get words with minimum threshold'''\n","    \n","    # count word\n","    word_counts = count_words(tokenized_sentences)\n","    closed_vocab = []\n","    \n","    # for each word and its count\n","    for word, cnt in word_counts.items():\n","        if cnt >= count_threshold:\n","            closed_vocab.append(word)\n","            \n","    return closed_vocab\n","\n","def replace_oov_words_by_unk(tokenized_sentences, vocabulary, unknown_token=\"<unk>\"):\n","    \"\"\"Replace words not in the given vocabulary with '<unk>' token\"\"\"\n","    \n","    vocabulary = set(vocabulary)\n","    replaced_tokenized_sentences = []\n","    \n","    # Go through each sentence\n","    for sentence in tokenized_sentences:\n","        replaced_sentence = []\n","        \n","        # for each token in the sentence\n","        for token in sentence:\n","            \n","            # Check if the token is in the closed vocabulary\n","            if token in vocabulary:\n","                replaced_sentence.append(token)\n","            else:\n","                replaced_sentence.append(unknown_token)\n","        \n","        # Append the list of tokens to the list of lists\n","        replaced_tokenized_sentences.append(replaced_sentence)\n","        \n","    return replaced_tokenized_sentences\n","\n","def preprocess_data(train_data, test_data, count_threshold):\n","    vocabulary = get_words_with_nplus_frequency(train_data, count_threshold)\n","    train_data_replaced = replace_oov_words_by_unk(train_data, vocabulary)\n","    test_data_replaced = replace_oov_words_by_unk(test_data, vocabulary)\n","    return train_data_replaced, test_data_replaced, vocabulary"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HEoYJvxpZREA"},"source":["sent = \"Sky is blue.\\nLeaves are green\\nRoses are red.\\nSky is the limit.\\nUnder the sky, over the blue ocean.\"\n","tokenized_sentences = get_tokenized_data(sent)\n","word_freq = count_words(tokenized_sentences)\n","tmp_closed_vocab = get_words_with_nplus_frequency(tokenized_sentences, count_threshold=2)\n","tmp_replaced_tokenized_sentences = replace_oov_words_by_unk(tokenized_sentences, tmp_closed_vocab)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g2lUs1xCZREC","executionInfo":{"status":"ok","timestamp":1618673135175,"user_tz":-480,"elapsed":81512,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"c93935e4-dc53-4f05-d865-bfcee333e7b9"},"source":["# train test split\n","tokenized_data = get_tokenized_data(data)\n","random.shuffle(tokenized_data)\n","\n","train_size = int(len(tokenized_data) * 0.8)\n","train_data = tokenized_data[0 : train_size]\n","test_data = tokenized_data[train_size : ]\n","print(len(train_data), len(test_data))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["38368 9593\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ArJJ6YhpZRED"},"source":["train_data_processed, test_data_processed, vocabulary = preprocess_data(train_data, test_data, count_threshold=2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S2dG_vmUZRED"},"source":["# N-gram based language models\n","## N-gram Propability\n","\n","In this section, you will develop the n-grams language model.\n","- Assume the probability of the next word depends only on the previous n-gram.\n","- The previous n-gram is the series of the previous 'n' words.\n","\n","The conditional probability for the word at position 't' in the sentence, given that the words preceding it are $w_{t-1}, w_{t-2} \\cdots w_{t-n}$ is:\n","\n","$$ P(w_t | w_{t-1}\\dots w_{t-n}) \\tag{1}$$\n","\n","You can estimate this probability  by counting the occurrences of these series of words in the training data.\n","- The probability can be estimated as a ratio, where\n","- The numerator is the number of times word 't' appears after words t-1 through t-n appear in the training data.\n","- The denominator is the number of times word t-1 through t-n appears in the training data.\n","\n","$$ \\hat{P}(w_t | w_{t-1}\\dots w_{t-n}) = \\frac{C(w_{t-1}\\dots w_{t-n}, w_n)}{C(w_{t-1}\\dots w_{t-n})} \\tag{2} $$\n","\n","- The function $C(\\cdots)$ denotes the number of occurence of the given sequence. \n","- $\\hat{P}$ means the estimation of $P$. \n","- Notice that denominator of the equation (2) is the number of occurence of the previous $n$ words, and the numerator is the same sequence followed by the word $w_t$.\n","\n","Later, you will modify the equation (2) by adding k-smoothing, which avoids errors when any counts are zero.\n","\n","The equation (2) tells us that to estimate probabilities based on n-grams, you need the counts of n-grams (for denominator) and (n+1)-grams (for numerator)."]},{"cell_type":"code","metadata":{"id":"GE2CxzsSZREE"},"source":["def count_n_grams(data, n, start_token='<s>', end_token = '<e>'):\n","    '''Count all n-grams in the data'''\n","    n_grams = defaultdict(int)\n","    \n","    # Go through each sentence in the data\n","    for sentence in data:\n","\n","        # prepend start token n times, and  append <e> one time\n","        sentence = [start_token] * n + sentence + [end_token]\n","        \n","        # convert list to tuple to be the key of dictionaries\n","        sentence = tuple(sentence)\n","        if n==1:\n","            m = len(sentence) \n","        else:\n","            m = len(sentence) - 1\n","        \n","        for i in range(m):\n","            n_gram = sentence[i : i + n]\n","            n_grams[n_gram] += 1\n","    return n_grams"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3UOxa8cxZREE","executionInfo":{"status":"ok","timestamp":1618673135905,"user_tz":-480,"elapsed":82220,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"56546208-164b-45bd-fe9e-8ed293f9c0b0"},"source":["sentences = [['i', 'like', 'a', 'cat'], ['this', 'dog', 'is', 'like', 'a', 'cat']]\n","print(\"Uni-gram:\")\n","print(count_n_grams(sentences, 1))\n","print(\"\\nBi-gram:\")\n","print(count_n_grams(sentences, 2))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Uni-gram:\n","defaultdict(<class 'int'>, {('<s>',): 2, ('i',): 1, ('like',): 2, ('a',): 2, ('cat',): 2, ('<e>',): 2, ('this',): 1, ('dog',): 1, ('is',): 1})\n","\n","Bi-gram:\n","defaultdict(<class 'int'>, {('<s>', '<s>'): 2, ('<s>', 'i'): 1, ('i', 'like'): 1, ('like', 'a'): 2, ('a', 'cat'): 2, ('cat', '<e>'): 2, ('<s>', 'this'): 1, ('this', 'dog'): 1, ('dog', 'is'): 1, ('is', 'like'): 1})\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bbbd2154ZREF"},"source":["<a name='ex-09'></a>\n","## Estimate Probability\n","\n","Next, estimate the probability of a word given the prior 'n' words using the n-gram counts.\n","\n","$$ \\hat{P}(w_t | w_{t-1}\\dots w_{t-n}) = \\frac{C(w_{t-1}\\dots w_{t-n}, w_n)}{C(w_{t-1}\\dots w_{t-n})} \\tag{2} $$\n","\n","This formula doesn't work when a count of an n-gram is zero..\n","- Suppose we encounter an n-gram that did not occur in the training data.  \n","- Then, the equation (2) cannot be evaluated (it becomes zero divided by zero).\n","\n","A way to handle zero counts is to add k-smoothing.  \n","- K-smoothing adds a positive constant $k$ to each numerator and $k \\times |V|$ in the denominator, where $|V|$ is the number of words in the vocabulary.\n","\n","$$ \\hat{P}(w_t | w_{t-1}\\dots w_{t-n}) = \\frac{C(w_{t-1}\\dots w_{t-n}, w_n) + k}{C(w_{t-1}\\dots w_{t-n}) + k|V|} \\tag{3} $$\n","\n","\n","For n-grams that have a zero count, the equation (3) becomes $\\frac{1}{|V|}$.\n","- This means that any n-gram with zero count has the same probability of $\\frac{1}{|V|}$.\n","\n","Define a function that computes the probability estimate (3) from n-gram counts and a constant $k$.\n","\n","- The function takes in a dictionary 'n_gram_counts', where the key is the n-gram and the value is the count of that n-gram.\n","- The function also takes another dictionary n_plus1_gram_counts, which you'll use to find the count for the previous n-gram plus the current word."]},{"cell_type":"code","metadata":{"id":"C8uZYqZjZREG"},"source":["def estimate_probability(next_word, previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n","    \"\"\"Estimate the probabilities of a next word using the n-gram counts with k-smoothing\"\"\"\n","    previous_n_gram = tuple(previous_n_gram)\n","    \n","    # Set the denominator\n","    previous_n_gram_count = n_gram_counts.get(previous_n_gram, 0)\n","    denominator = previous_n_gram_count + k * vocabulary_size\n","\n","    # Set numerator\n","    n_plus1_gram = previous_n_gram + (next_word,)\n","    n_plus1_gram_count = n_plus1_gram_counts.get(n_plus1_gram, 0)\n","    numerator = n_plus1_gram_count + k\n","    probability = numerator / denominator    \n","    return probability\n","\n","def estimate_probabilities(previous_n_gram, data, n, vocabulary, k=1.0):\n","    \"\"\"Estimate the probabilities of next words using the n-gram counts with k-smoothing\"\"\"\n","    \n","    n_gram_counts = count_n_grams(data, n)\n","    n_plus1_gram_counts = count_n_grams(data, n+1)\n","    previous_n_gram = tuple(previous_n_gram)\n","    \n","    # add <e> <unk> to the vocabulary, <s> is not needed since it should not appear as the next word\n","    vocabulary = vocabulary + [\"<e>\", \"<unk>\"]\n","    vocabulary_size = len(vocabulary)\n","    \n","    probabilities = {}\n","    for word in vocabulary:\n","        probability = estimate_probability(word, previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k)\n","        probabilities[word] = probability\n","    # probabilities = sorted(probabilities.items(), key=lambda x: x[1], reverse=True)\n","    return probabilities\n","\n","def make_count_matrix(n_plus1_gram_counts, vocabulary):\n","    vocabulary = vocabulary + [\"<e>\", \"<unk>\"]\n","    \n","    # obtain unique n-grams\n","    n_grams = []\n","    for n_plus1_gram in n_plus1_gram_counts.keys():\n","        n_gram = n_plus1_gram[0 : -1]\n","        n_grams.append(n_gram)\n","    n_grams = list(set(n_grams))\n","\n","    # mapping from n-gram to row\n","    row_index = {n_gram : i for i, n_gram in enumerate(n_grams)}\n","    \n","    # mapping from next word to column\n","    col_index = {word : j for j, word in enumerate(vocabulary)}\n","    \n","    nrow = len(n_grams)\n","    ncol = len(vocabulary)\n","    count_matrix = np.zeros((nrow, ncol))\n","    for n_plus1_gram, count in n_plus1_gram_counts.items():\n","        n_gram = n_plus1_gram[0:-1]\n","        word = n_plus1_gram[-1]\n","        if word not in vocabulary:\n","            continue\n","        i = row_index[n_gram]\n","        j = col_index[word]\n","        count_matrix[i, j] = count\n","    \n","    count_matrix = pd.DataFrame(count_matrix, index=n_grams, columns=vocabulary)\n","    return count_matrix\n","\n","def make_probability_matrix(n_plus1_gram_counts, vocabulary, k):\n","    count_matrix = make_count_matrix(n_plus1_gram_counts, unique_words)\n","    count_matrix += k\n","    prob_matrix = count_matrix.div(count_matrix.sum(axis=1), axis=0)\n","    return prob_matrix"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e3JbSlS4ZREM","executionInfo":{"status":"ok","timestamp":1618673135907,"user_tz":-480,"elapsed":82207,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"86ccd16e-c7be-4f6b-eb15-77309d8aec8b"},"source":["# test your code\n","sentences = [['i', 'like', 'a', 'cat'], ['this', 'dog', 'is', 'like', 'a', 'cat'], ['you', 'look', 'like', 'a', 'dog']]\n","unique_words = list(set(sentences[0] + sentences[1]))\n","estimate_probabilities(\"a\", sentences, 1, unique_words, k=1)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'<e>': 0.08333333333333333,\n"," '<unk>': 0.08333333333333333,\n"," 'a': 0.08333333333333333,\n"," 'cat': 0.25,\n"," 'dog': 0.16666666666666666,\n"," 'i': 0.08333333333333333,\n"," 'is': 0.08333333333333333,\n"," 'like': 0.08333333333333333,\n"," 'this': 0.08333333333333333}"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":483},"id":"t6KWfcwDZREN","executionInfo":{"status":"ok","timestamp":1618673135908,"user_tz":-480,"elapsed":82199,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"3a05cfe2-a684-49d8-9ce7-57c5d66868e3"},"source":["sentences = [['i', 'like', 'a', 'cat'], ['this', 'dog', 'is', 'like', 'a', 'cat']]\n","unique_words = list(set(sentences[0] + sentences[1]))\n","bigram_counts = count_n_grams(sentences, 5)\n","\n","display(make_count_matrix(bigram_counts, unique_words))"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>i</th>\n","      <th>cat</th>\n","      <th>like</th>\n","      <th>this</th>\n","      <th>a</th>\n","      <th>dog</th>\n","      <th>is</th>\n","      <th>&lt;e&gt;</th>\n","      <th>&lt;unk&gt;</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>(i, like, a, cat)</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>(&lt;s&gt;, &lt;s&gt;, &lt;s&gt;, i)</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>(a, cat)</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>(&lt;s&gt;, &lt;s&gt;, &lt;s&gt;, this)</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>(&lt;s&gt;, &lt;s&gt;, this, dog)</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>(this, dog, is, like)</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>(&lt;s&gt;, &lt;s&gt;, &lt;s&gt;, &lt;s&gt;)</th>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>(is, like, a, cat)</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>(like, a, cat)</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>(cat,)</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>(&lt;s&gt;, &lt;s&gt;, i, like)</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>(dog, is, like, a)</th>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>(&lt;s&gt;, this, dog, is)</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>(&lt;s&gt;, i, like, a)</th>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                         i  cat  like  this    a  dog   is  <e>  <unk>\n","(i, like, a, cat)      0.0  0.0   0.0   0.0  0.0  0.0  0.0  1.0    0.0\n","(<s>, <s>, <s>, i)     0.0  0.0   1.0   0.0  0.0  0.0  0.0  0.0    0.0\n","(a, cat)               0.0  0.0   0.0   0.0  0.0  0.0  0.0  2.0    0.0\n","(<s>, <s>, <s>, this)  0.0  0.0   0.0   0.0  0.0  1.0  0.0  0.0    0.0\n","(<s>, <s>, this, dog)  0.0  0.0   0.0   0.0  0.0  0.0  1.0  0.0    0.0\n","(this, dog, is, like)  0.0  0.0   0.0   0.0  1.0  0.0  0.0  0.0    0.0\n","(<s>, <s>, <s>, <s>)   1.0  0.0   0.0   1.0  0.0  0.0  0.0  0.0    0.0\n","(is, like, a, cat)     0.0  0.0   0.0   0.0  0.0  0.0  0.0  1.0    0.0\n","(like, a, cat)         0.0  0.0   0.0   0.0  0.0  0.0  0.0  2.0    0.0\n","(cat,)                 0.0  0.0   0.0   0.0  0.0  0.0  0.0  2.0    0.0\n","(<s>, <s>, i, like)    0.0  0.0   0.0   0.0  1.0  0.0  0.0  0.0    0.0\n","(dog, is, like, a)     0.0  1.0   0.0   0.0  0.0  0.0  0.0  0.0    0.0\n","(<s>, this, dog, is)   0.0  0.0   1.0   0.0  0.0  0.0  0.0  0.0    0.0\n","(<s>, i, like, a)      0.0  1.0   0.0   0.0  0.0  0.0  0.0  0.0    0.0"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":376},"id":"lKPYZJ78ZREO","executionInfo":{"status":"ok","timestamp":1618673135909,"user_tz":-480,"elapsed":82189,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"88f4ef8e-d1cc-4c53-f9f8-b6d813d48786"},"source":["sentences = [['i', 'like', 'a', 'cat'], ['this', 'dog', 'is', 'like', 'a', 'cat']]\n","unique_words = list(set(sentences[0] + sentences[1]))\n","bigram_counts = count_n_grams(sentences, 3)\n","print(\"Trigram probabilities\")\n","display(make_probability_matrix(bigram_counts, unique_words, k=1))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Trigram probabilities\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>i</th>\n","      <th>cat</th>\n","      <th>like</th>\n","      <th>this</th>\n","      <th>a</th>\n","      <th>dog</th>\n","      <th>is</th>\n","      <th>&lt;e&gt;</th>\n","      <th>&lt;unk&gt;</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>(&lt;s&gt;, i)</th>\n","      <td>0.100000</td>\n","      <td>0.100000</td>\n","      <td>0.200000</td>\n","      <td>0.100000</td>\n","      <td>0.100000</td>\n","      <td>0.100000</td>\n","      <td>0.100000</td>\n","      <td>0.100000</td>\n","      <td>0.100000</td>\n","    </tr>\n","    <tr>\n","      <th>(i, like)</th>\n","      <td>0.100000</td>\n","      <td>0.100000</td>\n","      <td>0.100000</td>\n","      <td>0.100000</td>\n","      <td>0.200000</td>\n","      <td>0.100000</td>\n","      <td>0.100000</td>\n","      <td>0.100000</td>\n","      <td>0.100000</td>\n","    </tr>\n","    <tr>\n","      <th>(this, dog)</th>\n","      <td>0.100000</td>\n","      <td>0.100000</td>\n","      <td>0.100000</td>\n","      <td>0.100000</td>\n","      <td>0.100000</td>\n","      <td>0.100000</td>\n","      <td>0.200000</td>\n","      <td>0.100000</td>\n","      <td>0.100000</td>\n","    </tr>\n","    <tr>\n","      <th>(&lt;s&gt;, &lt;s&gt;)</th>\n","      <td>0.181818</td>\n","      <td>0.090909</td>\n","      <td>0.090909</td>\n","      <td>0.181818</td>\n","      <td>0.090909</td>\n","      <td>0.090909</td>\n","      <td>0.090909</td>\n","      <td>0.090909</td>\n","      <td>0.090909</td>\n","    </tr>\n","    <tr>\n","      <th>(a, cat)</th>\n","      <td>0.090909</td>\n","      <td>0.090909</td>\n","      <td>0.090909</td>\n","      <td>0.090909</td>\n","      <td>0.090909</td>\n","      <td>0.090909</td>\n","      <td>0.090909</td>\n","      <td>0.272727</td>\n","      <td>0.090909</td>\n","    </tr>\n","    <tr>\n","      <th>(like, a)</th>\n","      <td>0.090909</td>\n","      <td>0.272727</td>\n","      <td>0.090909</td>\n","      <td>0.090909</td>\n","      <td>0.090909</td>\n","      <td>0.090909</td>\n","      <td>0.090909</td>\n","      <td>0.090909</td>\n","      <td>0.090909</td>\n","    </tr>\n","    <tr>\n","      <th>(&lt;s&gt;, this)</th>\n","      <td>0.100000</td>\n","      <td>0.100000</td>\n","      <td>0.100000</td>\n","      <td>0.100000</td>\n","      <td>0.100000</td>\n","      <td>0.200000</td>\n","      <td>0.100000</td>\n","      <td>0.100000</td>\n","      <td>0.100000</td>\n","    </tr>\n","    <tr>\n","      <th>(cat,)</th>\n","      <td>0.090909</td>\n","      <td>0.090909</td>\n","      <td>0.090909</td>\n","      <td>0.090909</td>\n","      <td>0.090909</td>\n","      <td>0.090909</td>\n","      <td>0.090909</td>\n","      <td>0.272727</td>\n","      <td>0.090909</td>\n","    </tr>\n","    <tr>\n","      <th>(is, like)</th>\n","      <td>0.100000</td>\n","      <td>0.100000</td>\n","      <td>0.100000</td>\n","      <td>0.100000</td>\n","      <td>0.200000</td>\n","      <td>0.100000</td>\n","      <td>0.100000</td>\n","      <td>0.100000</td>\n","      <td>0.100000</td>\n","    </tr>\n","    <tr>\n","      <th>(dog, is)</th>\n","      <td>0.100000</td>\n","      <td>0.100000</td>\n","      <td>0.200000</td>\n","      <td>0.100000</td>\n","      <td>0.100000</td>\n","      <td>0.100000</td>\n","      <td>0.100000</td>\n","      <td>0.100000</td>\n","      <td>0.100000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                    i       cat      like  ...        is       <e>     <unk>\n","(<s>, i)     0.100000  0.100000  0.200000  ...  0.100000  0.100000  0.100000\n","(i, like)    0.100000  0.100000  0.100000  ...  0.100000  0.100000  0.100000\n","(this, dog)  0.100000  0.100000  0.100000  ...  0.200000  0.100000  0.100000\n","(<s>, <s>)   0.181818  0.090909  0.090909  ...  0.090909  0.090909  0.090909\n","(a, cat)     0.090909  0.090909  0.090909  ...  0.090909  0.272727  0.090909\n","(like, a)    0.090909  0.272727  0.090909  ...  0.090909  0.090909  0.090909\n","(<s>, this)  0.100000  0.100000  0.100000  ...  0.100000  0.100000  0.100000\n","(cat,)       0.090909  0.090909  0.090909  ...  0.090909  0.272727  0.090909\n","(is, like)   0.100000  0.100000  0.100000  ...  0.100000  0.100000  0.100000\n","(dog, is)    0.100000  0.100000  0.200000  ...  0.100000  0.100000  0.100000\n","\n","[10 rows x 9 columns]"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"XTuJI-JdZREP"},"source":["# Perplexity\n","\n","In this section, you will generate the perplexity score to evaluate your model on the test set. \n","- You will also use back-off when needed. \n","- Perplexity is used as an evaluation metric of your language model. \n","- To calculate the  the perplexity score of the test set on an n-gram model, use: \n","\n","$$ PP(W) =\\sqrt[N]{ \\prod_{t=n+1}^N \\frac{1}{P(w_t | w_{t-n} \\cdots w_{t-1})} } \\tag{4}$$\n","\n","- where $N$ is the length of the sentence.\n","- $n$ is the number of words in the n-gram (e.g. 2 for a bigram).\n","- In math, the numbering starts at one and not zero.\n","\n","In code, array indexing starts at zero, so the code will use ranges for $t$ according to this formula:\n","\n","$$ PP(W) =\\sqrt[N]{ \\prod_{t=n}^{N-1} \\frac{1}{P(w_t | w_{t-n} \\cdots w_{t-1})} } \\tag{4.1}$$\n","\n","The higher the probabilities are, the lower the perplexity will be. \n","- The more the n-grams tell us about the sentence, the lower the perplexity score will be. "]},{"cell_type":"code","metadata":{"id":"kF7mvCBPZREP"},"source":["def calculate_perplexity(sentence, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n","    \"\"\"Calculate perplexity for a list of sentences\"\"\"\n","\n","    n = len(list(n_gram_counts.keys())[0]) \n","    sentence = [\"<s>\"] * n + sentence + [\"<e>\"]\n","    sentence = tuple(sentence)\n","    \n","    # length of sentence (after adding <s> and <e> tokens)\n","    N = len(sentence)\n","    product_pi = 1.0\n","        \n","    for t in range(n, N):\n","        # get the n-gram preceding the word at position t\n","        n_gram = sentence[t-n : t]\n","        \n","        # get the word at position t\n","        word = sentence[t]\n","                \n","        # Estimate the probability of the word given the n-gram\n","        probability = estimate_probability(word, n_gram, n_gram_counts, n_plus1_gram_counts, len(unique_words), k=1)\n","        \n","        # Update the product of the probabilities, 'product_pi' is a cumulative product \n","        product_pi *= 1 / probability\n","\n","    # Take the Nth root of the product\n","    perplexity = product_pi**(1/float(N))\n","    \n","    return perplexity"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cchVOJDPZREQ","executionInfo":{"status":"ok","timestamp":1618673135910,"user_tz":-480,"elapsed":82174,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"e7f4a037-1338-434a-ef76-e571c98d27a3"},"source":["sentences = [['i', 'like', 'a', 'cat'], ['this', 'dog', 'is', 'like', 'a', 'cat']]\n","unique_words = list(set(sentences[0] + sentences[1]))\n","\n","unigram_counts = count_n_grams(sentences, 1)\n","bigram_counts = count_n_grams(sentences, 2)\n","perplexity_train1 = calculate_perplexity(sentences[0], unigram_counts, bigram_counts, len(unique_words), k=1.0)\n","print(f\"Perplexity for first train sample: {perplexity_train1:.4f}\")\n","\n","test_sentence = ['i', 'like', 'a', 'dog']\n","perplexity_test = calculate_perplexity(test_sentence, unigram_counts, bigram_counts, len(unique_words), k=1.0)\n","print(f\"Perplexity for test sample: {perplexity_test:.4f}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Perplexity for first train sample: 2.8040\n","Perplexity for test sample: 3.9654\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"87FjXr95ZREQ"},"source":["## Perplexity of Seq Model"]},{"cell_type":"markdown","metadata":{"id":"SaABOW9TZRER"},"source":["The perplexity is a metric that measures how well a probability model predicts a sample and it is commonly used to evaluate language models. It is defined as: \n","\n","$$P(W) = \\sqrt[N]{\\prod_{i=1}^{N} \\frac{1}{P(w_i| w_1,...,w_{n-1})}}$$\n","\n","As an implementation hack, you would usually take the log of that formula (to enable us to use the log probabilities we get as output of our `RNN`, convert exponents to products, and products into sums which makes computations less complicated and computationally more efficient). You should also take care of the padding, since you do not want to include the padding when calculating the perplexity (because we do not want to have a perplexity measure artificially good). The algebra behind this process is explained next:\n","\n","\n","$$log P(W) = {log\\big(\\sqrt[N]{\\prod_{i=1}^{N} \\frac{1}{P(w_i| w_1,...,w_{n-1})}}\\big)}$$\n","\n","$$ = {log\\big({\\prod_{i=1}^{N} \\frac{1}{P(w_i| w_1,...,w_{n-1})}}\\big)^{\\frac{1}{N}}}$$ \n","\n","$$ = {log\\big({\\prod_{i=1}^{N}{P(w_i| w_1,...,w_{n-1})}}\\big)^{-\\frac{1}{N}}} $$\n","$$ = -\\frac{1}{N}{log\\big({\\prod_{i=1}^{N}{P(w_i| w_1,...,w_{n-1})}}\\big)} $$\n","$$ = -\\frac{1}{N}{\\big({\\sum_{i=1}^{N}{logP(w_i| w_1,...,w_{n-1})}}\\big)} $$"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4TEVgROVZRER","executionInfo":{"status":"ok","timestamp":1618673139304,"user_tz":-480,"elapsed":85559,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"6bf6016f-9a6b-4679-84dd-93f60d289da5"},"source":["# Load from .npy files\n","predictions = np.load('data/predictions.npy')\n","targets = np.load('data/targets.npy')\n","\n","# Cast to jax.interpreters.xla.DeviceArray\n","predictions = np.array(predictions)\n","targets = np.array(targets)\n","reshaped_targets = tl.one_hot(targets, predictions.shape[-1])\n","\n","# Print shapes\n","print(f'predictions has shape: {predictions.shape}')\n","print(f'targets has shape: {targets.shape}')\n","print(f'target has shape after reshapeing: {reshaped_targets.shape}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["predictions has shape: (32, 64, 256)\n","targets has shape: (32, 64)\n","target has shape after reshapeing: (32, 64, 256)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uJ7Wz-ejZRES"},"source":["total_log_ppx = np.sum(predictions * reshaped_targets, axis=-1)\n","# make non zero as 1\n","mask = 1.0 - np.equal(targets, 0)\n","real_log_ppx = total_log_ppx * mask\n","log_ppx = -(np.sum(real_log_ppx) / np.sum(mask))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"umkJKTsoZRES","executionInfo":{"status":"ok","timestamp":1618673139993,"user_tz":-480,"elapsed":86229,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"7cd36ce6-27da-48f8-e719-7ebd8c93b1d9"},"source":["print(f'The log perplexity and perplexity of the model are respectively: {log_ppx} and {np.exp(log_ppx)}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The log perplexity and perplexity of the model are respectively: 2.3281209468841553 and 10.258646965026855\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RdRhMoO6ZREU"},"source":["# Auto-complete system\n","\n","In this section, you will combine the language models developed so far to implement an auto-complete system. \n"]},{"cell_type":"code","metadata":{"id":"SqqEzrPAZREU"},"source":["def suggest_a_word(previous_tokens, data, n, vocabulary, k=1.0, start_with=None):\n","    \n","    # length of previous words\n","    n_gram_counts = count_n_grams(data, n)\n","    n_plus1_gram_counts = count_n_grams(data, n+1)\n","    \n","    # get the most recent 'n' words as the previous n-gram\n","    previous_n_gram = previous_tokens[-n:]\n","    \n","    # Estimate the probabilities that each word in the vocabulary given the previous n-gram\n","    probabilities = estimate_probabilities(previous_n_gram, data, n, vocabulary, k=k)\n","    \n","    # Initialize suggested word to None and prob to 0\n","    suggestion, max_prob = None, 0\n","        \n","    # For each word and its probability in the probabilities dictionary:\n","    for word, prob in probabilities.items():\n","        \n","        # If the optional start_with string is set\n","        if start_with != None:\n","            # Check if the beginning of word match with the letters in 'start_with', else skip\n","            if not word.startswith(start_with):\n","                continue  \n","        \n","        # Check if this word's probability is greater than the current maximum probability\n","        if prob > max_prob:\n","            suggestion = word\n","            max_prob = prob    \n","    return suggestion, max_prob\n","\n","def get_suggestions(previous_tokens, data, n_gram_counts_list, vocabulary, k=1.0, start_with=None):\n","    '''Get Multiple Suggestion'''\n","\n","    model_counts = len(n_gram_counts_list)\n","    suggestions = []\n","    \n","    for i in n_gram_counts_list:    \n","        suggestion = suggest_a_word(previous_tokens, data, i, vocabulary, k=k, start_with=start_with)\n","        suggestions.append(suggestion)\n","    return suggestions"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ag2LQwTtZREU","executionInfo":{"status":"ok","timestamp":1618673139995,"user_tz":-480,"elapsed":86219,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"b06ff29b-36bb-4922-96f3-36aaec2a7591"},"source":["# test your code\n","sentences = [['i', 'like', 'a', 'cat'], ['this', 'dog', 'is', 'like', 'a', 'cat']]\n","unique_words = list(set(sentences[0] + sentences[1]))\n","\n","previous_tokens = [\"i\", \"like\"]\n","tmp_suggest1 = suggest_a_word(previous_tokens, sentences, 2, unique_words, k=1.0)\n","print(tmp_suggest1)\n","\n","# test your code when setting the starts_with\n","previous_tokens = [\"i\", \"like\"]\n","tmp_starts_with = 'c'\n","tmp_suggest2 = suggest_a_word(previous_tokens, sentences, 2, unique_words, k=1.0, start_with=tmp_starts_with)\n","print(tmp_suggest2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["('a', 0.2)\n","('cat', 0.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":102},"id":"HLCuYfryZREV","executionInfo":{"status":"ok","timestamp":1618673139996,"user_tz":-480,"elapsed":86211,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"2665f04f-3dd7-4fdd-cc2b-1f5a17c2a6b6"},"source":["# test your code\n","sentences = [['i', 'like', 'a', 'cat'], ['this', 'dog', 'is', 'like', 'a', 'cat']]\n","unique_words = list(set(sentences[0] + sentences[1]))\n","\n","n_gram_counts_list = [2, 3, 4, 5, 6]\n","previous_tokens = ['you', 'know', \"i\", \"like\"]\n","tmp_suggest3 = get_suggestions(previous_tokens, sentences, n_gram_counts_list, unique_words, k=1.0)\n","\n","display(tmp_suggest3)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["[('a', 0.2),\n"," ('i', 0.1111111111111111),\n"," ('i', 0.1111111111111111),\n"," ('i', 0.1111111111111111),\n"," ('i', 0.1111111111111111)]"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"BCT2GmLtZREV"},"source":["## Suggest multiple words using n-grams of varying length"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":85},"id":"xwNKE4NAZREW","executionInfo":{"status":"ok","timestamp":1618673147365,"user_tz":-480,"elapsed":93570,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"aff17abf-05eb-4e9a-e3c6-b7a70f131cdd"},"source":["previous_tokens = [\"i\", \"am\", \"to\"]\n","n_gram_counts_list = [2, 3, 4]\n","tmp_suggest4 = get_suggestions(previous_tokens, train_data_processed, n_gram_counts_list, vocabulary, k=1.0)\n","\n","print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n","display(tmp_suggest4)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The previous words are ['i', 'am', 'to'], the suggestions are:\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/plain":["[('have', 0.0001343634531407457),\n"," ('have', 0.00013439956992137626),\n"," ('this', 6.721333512568894e-05)]"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":85},"id":"z-YvujLrZREX","executionInfo":{"status":"ok","timestamp":1618673154613,"user_tz":-480,"elapsed":100807,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"2c5e9bec-a23c-4192-dadd-a3a31e88a3d7"},"source":["n_gram_counts_list = [2, 3, 4]\n","previous_tokens = [\"i\", \"want\", \"to\", \"go\"]\n","tmp_suggest5 = get_suggestions(previous_tokens, train_data_processed, n_gram_counts_list, vocabulary, k=1.0)\n","\n","print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n","display(tmp_suggest5)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The previous words are ['i', 'want', 'to', 'go'], the suggestions are:\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/plain":["[('to', 0.004286750643012596),\n"," ('to', 0.0009389041647106163),\n"," ('to', 0.0004028738333445243)]"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":85},"id":"r2U5MofZZREY","executionInfo":{"status":"ok","timestamp":1618673161801,"user_tz":-480,"elapsed":107985,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"f135dd5f-7527-425a-a8ce-0d6a048c649f"},"source":["n_gram_counts_list = [2, 3, 4]\n","previous_tokens = [\"hey\", \"how\", \"are\"]\n","tmp_suggest6 = get_suggestions(previous_tokens, train_data_processed, n_gram_counts_list, vocabulary, k=1.0)\n","\n","print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n","display(tmp_suggest6)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The previous words are ['hey', 'how', 'are'], the suggestions are:\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/plain":["[('you', 0.00388011774150388),\n"," ('you', 0.0001344176355937899),\n"," ('this', 6.721333512568894e-05)]"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":85},"id":"C7hV71wMZREY","executionInfo":{"status":"ok","timestamp":1618673168913,"user_tz":-480,"elapsed":115087,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"422e16ca-716e-4026-f2e1-40839b8c53e1"},"source":["previous_tokens = [\"hey\", \"how\", \"are\", \"you\"]\n","tmp_suggest7 = get_suggestions(previous_tokens, train_data_processed, n_gram_counts_list, vocabulary, k=1.0)\n","\n","print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n","display(tmp_suggest7)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The previous words are ['hey', 'how', 'are', 'you'], the suggestions are:\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/plain":["[('?', 0.002881655642150763),\n"," ('?', 0.0016739203213927017),\n"," ('<e>', 0.0001344176355937899)]"]},"metadata":{"tags":[]}}]}]}