{"nbformat":4,"nbformat_minor":0,"metadata":{"coursera":{"course_slug":"neural-networks-deep-learning","graded_item_id":"XHpfv","launcher_item_id":"Zh0CU"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"A00 - Python_Basics_With_Numpy.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"3ztLcHtq_KxF"},"source":[" import math\n","import numpy as np\n","import time"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mUqf7EZd_kuE"},"source":["# Sigmoid"]},{"cell_type":"markdown","metadata":{"id":"PUN0Vkc2AD2t"},"source":["## Sigmoid Function"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qLm8ZeCb_KxG","executionInfo":{"status":"ok","timestamp":1634050915105,"user_tz":-480,"elapsed":19,"user":{"displayName":"Chan Chee Kean","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05792587367281359063"}},"outputId":"b6d1fd2c-e205-4538-d84b-da5ca6614573"},"source":["def basic_sigmoid(x):\n","    s = 1 / (1 + math.exp(-x))\n","    return s\n","basic_sigmoid(3)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9525741268224334"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"wZyFcpjp_KxH"},"source":["# error\n","x = np.array([1, 2, 3])\n","basic_sigmoid(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BVCdrBNo_KxI","executionInfo":{"status":"ok","timestamp":1621014593410,"user_tz":-480,"elapsed":985,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"2013cf04-7036-4930-d0b1-7ebeb9cb01f0"},"source":["x = np.array([1, 2, 3])\n","np.exp(x)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 2.71828183,  7.3890561 , 20.08553692])"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y-c6wN1a_KxI","executionInfo":{"status":"ok","timestamp":1621014603861,"user_tz":-480,"elapsed":947,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"da54c79c-ca54-4cc2-87e5-2582ce7425a4"},"source":["x = np.array([1, 2, 3])\n","x + 3"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([4, 5, 6])"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"eC_AFrPQ_KxJ"},"source":["def sigmoid(x):\n","    s = 1 / (1 + np.exp(-x))\n","    return s"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UV3qE9-f_KxJ","executionInfo":{"status":"ok","timestamp":1621014611989,"user_tz":-480,"elapsed":1017,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"0bfb4b8e-e6bb-43fc-f1cc-146b4412ebbe"},"source":["x = np.array([1, 2, 3])\n","sigmoid(x)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.73105858, 0.88079708, 0.95257413])"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"YjZL3GB7_KxK"},"source":["## Sigmoid gradient\n","\n","$$sigmoid\\_derivative(x) = \\sigma'(x) = \\sigma(x) (1 - \\sigma(x))$$\n","\n","Compute $\\sigma'(x) = s(1-s)$"]},{"cell_type":"code","metadata":{"id":"Lx1sjRYf_KxK"},"source":["def sigmoid_derivative(x):\n","    s = sigmoid(x)\n","    return s * (1-s"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sn_8QENK_KxK","executionInfo":{"status":"ok","timestamp":1621014750881,"user_tz":-480,"elapsed":1048,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"bdf346ce-02ca-43ff-d10c-0025cf0abc7c"},"source":["x = np.array([1, 2, 3])\n","print (\"sigmoid_derivative(x) = \" + str(sigmoid_derivative(x)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["sigmoid_derivative(x) = [0.19661193 0.10499359 0.04517666]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Bkm2mZksAm7n"},"source":["## Reshaping Array"]},{"cell_type":"code","metadata":{"id":"5vD5qncD_KxL","outputId":"52f23d76-4e8d-47ef-87bc-e45af03454b6"},"source":["# This is a 3 by 3 by 2 array, typically images will be (num_px_x, num_px_y, 3) where 3 represents the RGB values\n","image = np.array(\n","    [\n","        [\n","            [ 0.67826139,  0.29380381],\n","            [ 0.90714982,  0.52835647],\n","            [ 0.4215251 ,  0.45017551]\n","        ],\n","        \n","        [\n","            [ 0.92814219,  0.96677647],\n","            [ 0.85304703,  0.52351845],\n","            [ 0.19981397,  0.27417313]\n","        ],\n","        \n","        [\n","            [ 0.60659855,  0.00533165],\n","            [ 0.10820313,  0.49978937],\n","            [ 0.34144279,  0.94630077]\n","        ]\n","    ]\n",")\n","\n","image.reshape(-1, 1)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.67826139],\n","       [0.29380381],\n","       [0.90714982],\n","       [0.52835647],\n","       [0.4215251 ],\n","       [0.45017551],\n","       [0.92814219],\n","       [0.96677647],\n","       [0.85304703],\n","       [0.52351845],\n","       [0.19981397],\n","       [0.27417313],\n","       [0.60659855],\n","       [0.00533165],\n","       [0.10820313],\n","       [0.49978937],\n","       [0.34144279],\n","       [0.94630077]])"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"UZsfYy4w_KxM"},"source":["## Normalizing rows\n","\n","Another common technique we use in Machine Learning and Deep Learning is to normalize our data. It often leads to a better performance because gradient descent converges faster after normalization. Here, by normalization we mean changing x to $ \\frac{x}{\\| x\\|} $ (dividing each row vector of x by its norm).\n","\n","For example, if $$x = \n","\\begin{bmatrix}\n","    0 & 3 & 4 \\\\\n","    2 & 6 & 4 \\\\\n","\\end{bmatrix}\\tag{3}$$ then $$\\| x\\| = np.linalg.norm(x, axis = 1, keepdims = True) = \\begin{bmatrix}\n","    5 \\\\\n","    \\sqrt{56} \\\\\n","\\end{bmatrix}\\tag{4} $$and        $$ x\\_normalized = \\frac{x}{\\| x\\|} = \\begin{bmatrix}\n","    0 & \\frac{3}{5} & \\frac{4}{5} \\\\\n","    \\frac{2}{\\sqrt{56}} & \\frac{6}{\\sqrt{56}} & \\frac{4}{\\sqrt{56}} \\\\\n","\\end{bmatrix}\\tag{5}$$"]},{"cell_type":"code","metadata":{"id":"f-W8T-aj_KxN"},"source":["def normalizeRows(x):\n","    x_norm = np.linalg.norm(x, axis = 1, keepdims = True)\n","    x = x / x_norm\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6LRkAQs7_KxN","executionInfo":{"status":"ok","timestamp":1621014829974,"user_tz":-480,"elapsed":1065,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"039e7cf5-9354-4b0f-90da-02bc02ed4f27"},"source":["x = np.array([\n","    [0, 3, 4],\n","    [1, 6, 4]])\n","normalizeRows(x)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.        , 0.6       , 0.8       ],\n","       [0.13736056, 0.82416338, 0.54944226]])"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"jmDLqPIU_KxO"},"source":["## Broadcasting\n","A very important concept to understand in numpy is \"broadcasting\". It is very useful for performing mathematical operations between arrays of different shapes."]},{"cell_type":"code","metadata":{"id":"qS1RUsRm_KxP"},"source":["def softmax(x):\n","    x_exp = np.exp(x) # (2, 5)\n","    x_sum = np.sum(x_exp, axis=1, keepdims=True) # (2, 1)\n","    s = x_exp / x_sum\n","    return s"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"THafkwvB_KxP","executionInfo":{"status":"ok","timestamp":1634050915107,"user_tz":-480,"elapsed":13,"user":{"displayName":"Chan Chee Kean","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05792587367281359063"}},"outputId":"c5e59ac7-dc15-4cfb-e5d2-6f7416c739c0"},"source":["x = np.array([\n","    [9, 2, 5, 0, 0],\n","    [7, 5, 0, 0 ,0]])\n","softmax(x)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[9.80897665e-01, 8.94462891e-04, 1.79657674e-02, 1.21052389e-04,\n","        1.21052389e-04],\n","       [8.78679856e-01, 1.18916387e-01, 8.01252314e-04, 8.01252314e-04,\n","        8.01252314e-04]])"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"-yfULtfJ_KxQ"},"source":["# Vectorization"]},{"cell_type":"markdown","metadata":{"id":"55PZpJdm_KxQ"},"source":["## Vectorized Operation\n","\n","In deep learning, you deal with very large datasets. Hence, a non-computationally-optimal function can become a huge bottleneck in your algorithm and can result in a model that takes ages to run. To make sure that your code is  computationally efficient, you will use vectorization. the vectorized implementation is much cleaner and more efficient. For bigger vectors/matrices,"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wHN2uTwc_KxQ","executionInfo":{"status":"ok","timestamp":1621015036644,"user_tz":-480,"elapsed":1043,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"df45c591-e699-4a5f-c7dc-8678b123ab9b"},"source":["x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\n","x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n","\n","### CLASSIC DOT PRODUCT OF VECTORS IMPLEMENTATION ###\n","tic = time.process_time()\n","dot = 0\n","for i in range(len(x1)):\n","    dot+= x1[i] * x2[i]\n","toc = time.process_time()\n","print (\"dot = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n","\n","### CLASSIC OUTER PRODUCT IMPLEMENTATION ###\n","tic = time.process_time()\n","outer = np.zeros((len(x1), len(x2))) # we create a len(x1)*len(x2) matrix with only zeros\n","for i in range(len(x1)):\n","    for j in range(len(x2)):\n","        outer[i,j] = x1[i]*x2[j]\n","toc = time.process_time()\n","print (\"outer = \" + str(outer) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n","\n","### CLASSIC ELEMENTWISE IMPLEMENTATION ###\n","tic = time.process_time()\n","mul = np.zeros(len(x1))\n","for i in range(len(x1)):\n","    mul[i] = x1[i]*x2[i]\n","toc = time.process_time()\n","print (\"elementwise multiplication = \" + str(mul) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n","\n","### CLASSIC GENERAL DOT PRODUCT IMPLEMENTATION ###\n","W = np.random.rand(3,len(x1)) # Random 3*len(x1) numpy array\n","tic = time.process_time()\n","gdot = np.zeros(W.shape[0])\n","for i in range(W.shape[0]):\n","    for j in range(len(x1)):\n","        gdot[i] += W[i,j]*x1[j]\n","toc = time.process_time()\n","print (\"gdot = \" + str(gdot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["dot = 278\n"," ----- Computation time = 0.08183999999999969ms\n","outer = [[81. 18. 18. 81.  0. 81. 18. 45.  0.  0. 81. 18. 45.  0.  0.]\n"," [18.  4.  4. 18.  0. 18.  4. 10.  0.  0. 18.  4. 10.  0.  0.]\n"," [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]\n"," [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"," [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"," [63. 14. 14. 63.  0. 63. 14. 35.  0.  0. 63. 14. 35.  0.  0.]\n"," [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]\n"," [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"," [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"," [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"," [81. 18. 18. 81.  0. 81. 18. 45.  0.  0. 81. 18. 45.  0.  0.]\n"," [18.  4.  4. 18.  0. 18.  4. 10.  0.  0. 18.  4. 10.  0.  0.]\n"," [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]\n"," [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"," [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n"," ----- Computation time = 0.25239999999993046ms\n","elementwise multiplication = [81.  4. 10.  0.  0. 63. 10.  0.  0.  0. 81.  4. 25.  0.  0.]\n"," ----- Computation time = 0.09647999999984336ms\n","gdot = [14.48403098 30.55886451 27.4760059 ]\n"," ----- Computation time = 0.16004999999985614ms\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f0ZnPgxJ_KxQ","executionInfo":{"status":"ok","timestamp":1621015037731,"user_tz":-480,"elapsed":970,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"f616fa4e-0595-4e76-f598-e93e20cdb394"},"source":["x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\n","x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n","\n","### VECTORIZED DOT PRODUCT OF VECTORS ###\n","tic = time.process_time()\n","dot = np.dot(x1,x2)\n","toc = time.process_time()\n","print (\"dot = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n","\n","### VECTORIZED OUTER PRODUCT ###\n","tic = time.process_time()\n","outer = np.outer(x1,x2)\n","toc = time.process_time()\n","print (\"outer = \" + str(outer) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n","\n","### VECTORIZED ELEMENTWISE MULTIPLICATION ###\n","tic = time.process_time()\n","mul = np.multiply(x1,x2)\n","toc = time.process_time()\n","print (\"elementwise multiplication = \" + str(mul) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n","\n","### VECTORIZED GENERAL DOT PRODUCT ###\n","tic = time.process_time()\n","dot = np.dot(W,x1)\n","toc = time.process_time()\n","print (\"gdot = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["dot = 278\n"," ----- Computation time = 0.10369000000043371ms\n","outer = [[81 18 18 81  0 81 18 45  0  0 81 18 45  0  0]\n"," [18  4  4 18  0 18  4 10  0  0 18  4 10  0  0]\n"," [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n"," [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"," [63 14 14 63  0 63 14 35  0  0 63 14 35  0  0]\n"," [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n"," [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"," [81 18 18 81  0 81 18 45  0  0 81 18 45  0  0]\n"," [18  4  4 18  0 18  4 10  0  0 18  4 10  0  0]\n"," [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n"," [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n"," ----- Computation time = 0.08816999999972097ms\n","elementwise multiplication = [81  4 10  0  0 63 10  0  0  0 81  4 25  0  0]\n"," ----- Computation time = 0.05176000000073344ms\n","gdot = [14.48403098 30.55886451 27.4760059 ]\n"," ----- Computation time = 0.11989999999961753ms\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Srsz8FEM_KxR"},"source":["## IL1 and L2 loss functions\n","\n","- L1 loss is defined as:\n","$$\\begin{align*} & L_1(\\hat{y}, y) = \\sum_{i=0}^m|y^{(i)} - \\hat{y}^{(i)}| \\end{align*}\\tag{6}$$"]},{"cell_type":"code","metadata":{"id":"3qxblfOS_KxR"},"source":["def L1(yhat, y):\n","    loss = np.sum(abs(y-yhat))\n","    return loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QGOlwsmH_KxR","executionInfo":{"status":"ok","timestamp":1621015127317,"user_tz":-480,"elapsed":850,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"002912ff-6114-4970-8cd6-f5ee8e23ccac"},"source":["yhat = np.array([.9, 0.2, 0.1, .4, .9])\n","y = np.array([1, 0, 0, 1, 1])\n","L1(yhat,y)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1.1"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"markdown","metadata":{"id":"rsNMK4mw_KxR"},"source":["- L2 loss is defined as $$\\begin{align*} & L_2(\\hat{y},y) = \\sum_{i=0}^m(y^{(i)} - \\hat{y}^{(i)})^2 \\end{align*}\\tag{7}$$"]},{"cell_type":"code","metadata":{"id":"t-L8eUU3_KxS"},"source":["def L2(yhat, y):\n","    loss = np.sum(np.dot(y-yhat, y-yhat))    \n","    return loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5CZRV2W0_KxS","executionInfo":{"status":"ok","timestamp":1621015156996,"user_tz":-480,"elapsed":1106,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"c40bf7b1-b9f9-42ab-ae0d-bce0efc1822f"},"source":["yhat = np.array([.9, 0.2, 0.1, .4, .9])\n","y = np.array([1, 0, 0, 1, 1])\n","L2(yhat,y)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.43"]},"metadata":{"tags":[]},"execution_count":36}]}]}